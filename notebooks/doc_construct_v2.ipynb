{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import base64\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "import codecs\n",
    "import json\n",
    "from itertools import groupby\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = '/Users/kd/Workspace/python/DOCX/document-formatting/data/input/long_paragraph.docx'\n",
    "output_dir     = '/Users/kd/Workspace/python/DOCX/document-formatting/data/output'\n",
    "\n",
    "fetch_content_filepath = '/Users/kd/Workspace/python/DOCX/document-formatting/data/input/long_paragraph.json'\n",
    "filename       = os.path.splitext(os.path.basename(input_filepath))[0]\n",
    "translated_filename = filename + '_translated' + '.docx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_xmltree(xml):\n",
    "    return etree.tostring(xml)\n",
    "\n",
    "def get_xml_tree(xml_string):\n",
    "    return etree.fromstring(xml_string)\n",
    "\n",
    "def get_xmltree(filepath, parse='xml'):\n",
    "    if parse == 'html':\n",
    "        parser = etree.HTMLParser()\n",
    "        tree   = etree.parse(open(filepath, mode='r', encoding='utf-8'), parser)\n",
    "        return tree\n",
    "    else:\n",
    "        with open(filepath,'r') as file:\n",
    "            xml_string    = file.read()\n",
    "            return etree.fromstring(bytes(xml_string, encoding='utf-8'))\n",
    "    return None\n",
    "\n",
    "def check_element_is(element, type_char):\n",
    "    word_schema1 = \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"\n",
    "    word_schema2 = 'http://purl.oclc.org/ooxml/wordprocessingml/main'\n",
    "     \n",
    "    return (element.tag == '{%s}%s' % (word_schema1, type_char)) or (element.tag == '{%s}%s' % (word_schema2, type_char))\n",
    "\n",
    "def get_specific_tags(node, type_char):\n",
    "    nodes = []\n",
    "    for elem in node.iter():\n",
    "        if check_element_is(elem, type_char):\n",
    "            nodes.append(elem)\n",
    "    return nodes\n",
    "\n",
    "def add_identifier(node):\n",
    "    node.attrib['id'] = str(uuid.uuid4())\n",
    "\n",
    "def is_run_superscript(run):\n",
    "    attrib    = {}\n",
    "    vertAlign = get_specific_tags(run, 'vertAlign')\n",
    "    if len(vertAlign) > 0:\n",
    "        for key in vertAlign[0].attrib.keys():\n",
    "            attrib['vertAlign_' + key.split('}')[-1]] = vertAlign[0].attrib[key]\n",
    "    if 'vertAlign_val' in attrib:\n",
    "        if attrib['vertAlign_val'] == 'superscript':\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def update_run_text(r1, r2):\n",
    "    t1s = get_specific_tags(r1, 't')\n",
    "    t2s = get_specific_tags(r2, 't')\n",
    "#     print('r1 text [%s], r2 text [%s]'% (t1s[0].text, t2s[0].text))\n",
    "    t1s[0].text = t1s[0].text + t2s[0].text\n",
    "    t2s[0].text = ''\n",
    "    \n",
    "def get_run_properties(run):\n",
    "    attrib = {}\n",
    "    rFonts = get_specific_tags(run, 'rFonts')\n",
    "    sz     = get_specific_tags(run, 'sz')\n",
    "    szCs   = get_specific_tags(run, 'szCs')\n",
    "    \n",
    "    if len(rFonts) > 0:\n",
    "        for key in rFonts[0].attrib.keys():\n",
    "            attrib['rFonts_' + key.split('}')[-1]] = rFonts[0].attrib[key]\n",
    "    \n",
    "    if len(sz) > 0:\n",
    "        for key in sz[0].attrib.keys():\n",
    "            attrib['sz_' + key.split('}')[-1]] = sz[0].attrib[key]\n",
    "        \n",
    "    if len(szCs) > 0:\n",
    "        for key in szCs[0].attrib.keys():\n",
    "            attrib['szCs_' + key.split('}')[-1]] = szCs[0].attrib[key]\n",
    "\n",
    "    return attrib\n",
    "\n",
    "def update_font_property(p, reduce=4):    \n",
    "    szs    = get_specific_tags(p, 'sz')\n",
    "    szCss  = get_specific_tags(p, 'szCs')\n",
    "    value  = '{%s}%s' % (\"http://schemas.openxmlformats.org/wordprocessingml/2006/main\", 'val')\n",
    "\n",
    "    for szCs in szCss:\n",
    "        size  = szCs.attrib[value]\n",
    "        szCs.set(value, str(int(size) - reduce))\n",
    "\n",
    "    for sz in szs:\n",
    "        size  = sz.attrib[value]\n",
    "        sz.set(value, str(int(size) - reduce))\n",
    "    \n",
    "def compare_run_properties(run1, run2):\n",
    "    attrib1 = get_run_properties(run1)\n",
    "    attrib2 = get_run_properties(run2)\n",
    "    \n",
    "    if all (k in attrib1 for k in ('rFonts_ascii', 'sz_val', 'szCs_val')):\n",
    "        if all (k in attrib2 for k in ('rFonts_ascii', 'sz_val', 'szCs_val')):\n",
    "            if (attrib1['rFonts_ascii'] == attrib2['rFonts_ascii']) and \\\n",
    "            (attrib1['szCs_val'] == attrib2['szCs_val']) and \\\n",
    "            (attrib1['sz_val'] == attrib2['sz_val']) :\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_line_connections(p):\n",
    "    runs             = get_specific_tags(p, 'r')\n",
    "    text_runs        = []\n",
    "    \n",
    "    for run in runs:\n",
    "        if is_run_superscript(run) == False:\n",
    "            text_runs.append(run)\n",
    "\n",
    "    line_connections = []\n",
    "    for index in range(len(text_runs) - 1):\n",
    "        if (compare_run_properties(text_runs[index], text_runs[index+1])):\n",
    "            line_connections.append((index, index+1, 'CONNECTED'))\n",
    "        else:\n",
    "            line_connections.append((index, index+1, 'NOT_CONNECTED'))\n",
    "    return line_connections\n",
    "\n",
    "def arrange_grouped_line_indices(line_connections, debug=False):\n",
    "    lines          = [list(i) for j, i in groupby(line_connections, lambda a: a[2])]\n",
    "    if debug:\n",
    "        print('arrange_grouped_line_indices: %s \\n---------\\n' % (str(lines)))\n",
    "        \n",
    "    arranged_lines = []\n",
    "\n",
    "    for line_items in lines:\n",
    "        indices = []\n",
    "        for line_item in line_items:\n",
    "            indices.append(line_item[0])\n",
    "            indices.append(line_item[1])\n",
    "        indices = sorted(list(set(indices)))\n",
    "        arranged_lines.append([indices, line_items[0][2]])\n",
    "        \n",
    "    if debug:\n",
    "        print('arrange_grouped_line_indices,arranged_lines : %s \\n---------\\n' % (str(arranged_lines)))\n",
    "    \n",
    "    final_arranged_lines = []\n",
    "    \n",
    "    if len(arranged_lines) == 1:\n",
    "        final_arranged_lines.append([arranged_lines[0][0], arranged_lines[0][1]])\n",
    "    else:\n",
    "        for index, line_item in enumerate(arranged_lines):\n",
    "            if index == 0 and line_item[1] == 'NOT_CONNECTED':\n",
    "                del line_item[0][-1]\n",
    "            if index > 0 and index < (len(arranged_lines) - 1) and line_item[1] == 'NOT_CONNECTED':\n",
    "                del line_item[0][0]\n",
    "                del line_item[0][-1]\n",
    "            if index == (len(arranged_lines) - 1) and line_item[1] == 'NOT_CONNECTED':\n",
    "                del line_item[0][0]\n",
    "\n",
    "            final_arranged_lines.append([line_item[0], line_item[1]])\n",
    "    if debug:\n",
    "        print('final_arrange_grouped_line_indices,arranged_lines : %s \\n---------\\n' % (str(final_arranged_lines)))\n",
    "            \n",
    "    return final_arranged_lines\n",
    "\n",
    "def merge_runs(node, grouped_runs, debug=False):\n",
    "    runs      = get_specific_tags(node, 'r')\n",
    "    text_runs = []\n",
    "    \n",
    "    for run in runs:\n",
    "        if is_run_superscript(run) == False:\n",
    "            text_runs.append(run)\n",
    "\n",
    "    for element in grouped_runs:\n",
    "        if (element[1] == 'CONNECTED'):\n",
    "            for index, run_index in enumerate(element[0]):\n",
    "                if (index > 0):\n",
    "                    if (debug):\n",
    "                        print('merge index %d with %d' % ( run_index, 0))\n",
    "                    update_run_text(text_runs[0], text_runs[run_index])\n",
    "                    text_runs[run_index].getparent().remove(text_runs[run_index])\n",
    "                    \n",
    "def update_document_runs(document):\n",
    "    '''\n",
    "    the function iterates through the p tags and merges run that have exactly same\n",
    "    visual property.\n",
    "    '''\n",
    "    tag_name                 = 'p'\n",
    "    tags                     = get_specific_tags(document, tag_name)\n",
    "    for p in tags:\n",
    "        grouped_runs = arrange_grouped_line_indices(get_line_connections(p))\n",
    "        merge_runs(p, grouped_runs, debug=False)\n",
    "    return document\n",
    "\n",
    "def get_text_tags(document):\n",
    "    tags         = []\n",
    "    runs         = get_specific_tags(document, 'r')\n",
    "    for run in runs:\n",
    "        if is_run_superscript(run) == False:\n",
    "            texts = get_specific_tags(run, 't')\n",
    "            for text in texts:\n",
    "                if text.text and len(text.text.strip()) > 0:\n",
    "                    add_identifier(text)\n",
    "                    tags.append(text)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docx(filepath, working_dir):\n",
    "    filename       = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    extract_dir    = os.path.join(working_dir, filename)\n",
    "    \n",
    "    with ZipFile(filepath, 'r') as file:\n",
    "        file.extractall(path=extract_dir)\n",
    "        filenames = file.namelist()\n",
    "    \n",
    "    return extract_dir, filenames\n",
    "\n",
    "def save_docx(extracted_dir, filenames, output_filename):\n",
    "    with ZipFile(output_filename, 'w') as docx:\n",
    "        for filename in filenames: \n",
    "            docx.write(os.path.join(extracted_dir, filename), filename)\n",
    "            \n",
    "def save_document_xml(extracted_dir, xml):\n",
    "    with open(os.path.join(extracted_dir,'word/document.xml'), 'wb') as f:\n",
    "        xmlstr = get_string_xmltree(xml)\n",
    "        f.write(xmlstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(filepath):\n",
    "    from jsonpath_rw import jsonpath, parse\n",
    "    json_data     = json.load(codecs.open(fetch_content_filepath, 'r', 'utf-8-sig'))\n",
    "    jsonpath_expr = parse('$..tokenized_sentences[*]')\n",
    "    matches       = jsonpath_expr.find(json_data)\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    for match in matches:\n",
    "        tokenized_sentences.append(match.value)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences(string, substring): \n",
    "    count = 0\n",
    "    start = 0\n",
    "    while start < len(string): \n",
    "        pos = string.find(substring, start) \n",
    "\n",
    "        if pos != -1: \n",
    "            start = pos + 1\n",
    "            count += 1\n",
    "        else: \n",
    "            break\n",
    "    return count \n",
    "\n",
    "\n",
    "def check_string_status(doc_tag, tokenized):\n",
    "    doc_text       = doc_tag.text.replace(\" \", \"\")\n",
    "    tokenized_text = tokenized['src'].replace(\" \", \"\")\n",
    "    \n",
    "    if len(doc_text) < 2 or len(tokenized_text) < 2:\n",
    "        if doc_text.isdigit() == False or tokenized_text.isdigit() == False:\n",
    "            return (False, False)\n",
    "    \n",
    "    '''\n",
    "        perfect match\n",
    "    '''\n",
    "    if doc_text == tokenized_text:\n",
    "        return (True, 0)\n",
    "    \n",
    "    count         = 0\n",
    "    if len(doc_text) > len(tokenized_text):\n",
    "        count = count_occurrences(doc_text, tokenized_text)\n",
    "        if count != 0:\n",
    "            return (True, -1)\n",
    "    else:\n",
    "        count = count_occurrences(tokenized_text, doc_text)\n",
    "        if count != 0:\n",
    "            return (True, 1)\n",
    "    \n",
    "    return (False, False)\n",
    "\n",
    "def string_overlap(str1, str2):\n",
    "    str1_list   = [x for x in str1.split(' ') if x]\n",
    "    str1_set    = set(str1_list)\n",
    "    \n",
    "    str2_list   = [x for x in str2.split(' ') if x]\n",
    "    str2_set    = set(str2_list)\n",
    "    \n",
    "    common_set  = str1_set.intersection(str2_set)\n",
    "    diff_set    = str1_set.difference(str2_set)\n",
    "    overlap_list = []\n",
    "    \n",
    "    if len(str1_list) > len(str2_list):\n",
    "        for word in str2_list:\n",
    "            if word in list(common_set):\n",
    "                overlap_list.append(word)\n",
    "    else:\n",
    "        for word in str1_list:\n",
    "            if word in list(common_set):\n",
    "                overlap_list.append(word)\n",
    "                \n",
    "    return ' '.join(overlap_list)\n",
    "\n",
    "def check_string_status_v1(doc_tag, tokenized, overlap_threshold=4):\n",
    "    doc_text       = doc_tag.text.replace(\" \", \"\")\n",
    "    tokenized_text = tokenized['src'].replace(\" \", \"\")\n",
    "    \n",
    "    if len(doc_text) < 2 or len(tokenized_text) < 2:\n",
    "        if doc_text.isdigit() == False or tokenized_text.isdigit() == False:\n",
    "            return (False, False)\n",
    "    \n",
    "    '''\n",
    "        perfect match\n",
    "    '''\n",
    "    if doc_text == tokenized_text:\n",
    "        return (True, 0)\n",
    "    \n",
    "    \n",
    "    doc_text              = doc_tag.text\n",
    "    tokenized_text        = tokenized['src']\n",
    "    overlap, start, end, percentage, smaller = get_overlap(doc_text, tokenized_text)\n",
    "\n",
    "#     overlap_str_list      = [x for x in overlap_str.split(' ') if x]\n",
    "#     doc_text_list         = [x for x in doc_text.split(' ') if x]\n",
    "#     tokenized_text_list   = [x for x in tokenized_text.split(' ') if x]\n",
    "    \n",
    "#     if len(overlap_str) > 0:\n",
    "#         if (len(doc_text_list) <= len(tokenized_text_list)):\n",
    "#             if (abs(len(doc_text_list) - len(overlap_str_list)) <= overlap_threshold):\n",
    "#                 return (True, 1)\n",
    "#         else:\n",
    "#             if (abs(len(tokenized_text_list) - len(overlap_str_list)) <= overlap_threshold):\n",
    "#                 return (True, -1)\n",
    "#     else:\n",
    "#         '''\n",
    "#          when sentence overlap is not found, trying overlap at character level\n",
    "#         '''\n",
    "#         count         = 0\n",
    "#         if len(doc_text) > len(tokenized_text):\n",
    "#             count = count_occurrences(doc_text, tokenized_text)\n",
    "#             if count != 0:\n",
    "#                 return (True, -1)\n",
    "#         else:\n",
    "#             count = count_occurrences(tokenized_text, doc_text)\n",
    "#             if count != 0:\n",
    "#                 return (True, 1)\n",
    "                \n",
    "    \n",
    "#     return (False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_as_df(tags, tokenized_sentences):\n",
    "    doc_texts = []\n",
    "    doc_ids   = []\n",
    "    for tag in tags:\n",
    "        doc_texts.append(tag.text)\n",
    "        doc_ids.append(tag.attrib['id'])\n",
    "\n",
    "    tokenized_src_texts = []\n",
    "    tokenized_tgt_texts = []\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        tokenized_src_texts.append(tokenized_sentence['src'])\n",
    "        tokenized_tgt_texts.append(tokenized_sentence['tgt'])\n",
    "\n",
    "    if len(doc_texts) > len(tokenized_src_texts):\n",
    "        empty = [''] * (len(doc_texts) - len(tokenized_src_texts))\n",
    "        tokenized_src_texts.extend(empty)\n",
    "        tokenized_tgt_texts.extend(empty)\n",
    "    else:\n",
    "        empty = [''] * (len(tokenized_src_texts) - len(doc_texts))\n",
    "        doc_texts.extend(empty)\n",
    "        doc_ids.extend(empty)\n",
    "\n",
    "    df = pd.DataFrame(list(zip(doc_texts, doc_ids, tokenized_src_texts, tokenized_tgt_texts)), \n",
    "                                  columns =['doc_texts', 'doc_ids', 'tokenized_src_texts', 'tokenized_tgt_texts'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_tokenized_text_matched(df):\n",
    "    '''\n",
    "     return df, df1, creates merged string of tokenized sentences that are equivalent to document text\n",
    "    '''\n",
    "    df1 = []\n",
    "    for doc_id in df['doc_id'].unique():\n",
    "        temp_df = df[df['doc_id'] == doc_id]\n",
    "\n",
    "        src_text = ''\n",
    "        tgt_text = ''\n",
    "        for index, row in temp_df.iterrows():\n",
    "            src_text = src_text + row['src'] + ' '\n",
    "            tgt_text = tgt_text + row['tgt'] + ' '\n",
    "\n",
    "        temp_df_first = temp_df[0:1].reset_index(drop=True)\n",
    "        temp_df_first.at[0, 'src'] = src_text\n",
    "        temp_df_first.at[0, 'tgt'] = tgt_text\n",
    "\n",
    "        df1.append(temp_df_first)\n",
    "    \n",
    "    return pd.concat(df1)\n",
    "\n",
    "def get_updated_document_text_matched(df):\n",
    "    '''\n",
    "     returns two dfs, df3: contains text that are actually substring of tokenized sentence\n",
    "     df4: are multiple occurance text that have same spelling.\n",
    "    '''\n",
    "    df1 = []\n",
    "    df2 = []\n",
    "\n",
    "    for s_id in df['s_id'].unique():\n",
    "        sid_df = df[df['s_id'] == s_id].reset_index(drop=True)\n",
    "\n",
    "        doc_text = ''\n",
    "        for index, row in sid_df.iterrows():\n",
    "            doc_text = doc_text + row['doc_text'] + ' '\n",
    "\n",
    "        doc_text_list         = [x for x in doc_text.split(' ') if x]\n",
    "        sid_src_text_list     = [x for x in sid_df.iloc[0]['src'].split(' ') if x]\n",
    "\n",
    "        if len(doc_text_list) <= len(sid_src_text_list):\n",
    "            for index, row in sid_df.iterrows():\n",
    "                if index > 0:\n",
    "                    sid_df.at[index, 'tgt'] = ''\n",
    "            df1.append(sid_df)\n",
    "        else:\n",
    "            df2.append(sid_df)\n",
    "\n",
    "    df3 = pd.DataFrame()\n",
    "    df4 = pd.DataFrame()\n",
    "    if len(df1) > 0:\n",
    "        df3 = pd.concat(df1).reset_index(drop=True)\n",
    "    \n",
    "    if len(df2) > 0:\n",
    "        df4 = pd.concat(df2).drop_duplicates(subset='doc_id', keep='first').reset_index(drop=True)\n",
    "    \n",
    "    return df3, df4\n",
    "\n",
    "def replace_translated_df(df, texts):\n",
    "    for index, row in df.iterrows():\n",
    "        for text in texts:\n",
    "            if 'id' in text.attrib:\n",
    "                if text.attrib['id'] == row['doc_id']:\n",
    "                    text.text = row['tgt']\n",
    "                    \n",
    "                    \n",
    "def get_matched_dfs(tokenized_sentences, texts):\n",
    "    matched_ids    = []\n",
    "    matched_sids   = []\n",
    "\n",
    "    founds         = []\n",
    "    is_substrings  = []\n",
    "    doc_texts      = []\n",
    "    srcs           = []\n",
    "    tgts           = []\n",
    "    ids            = []\n",
    "    s_ids          = []\n",
    "\n",
    "    for sent_index in range(len(tokenized_sentences)):\n",
    "        for text_index in range(len(texts)):\n",
    "            if (texts[text_index].attrib['id'] in matched_ids) or \\\n",
    "            (tokenized_sentences[sent_index]['s_id'] in matched_sids):\n",
    "                continue\n",
    "\n",
    "            is_found, is_substring = check_string_status_v1(texts[text_index], tokenized_sentences[sent_index])\n",
    "\n",
    "            if is_found and is_substring == 0:\n",
    "                matched_ids.append(texts[text_index].attrib['id'])\n",
    "                matched_sids.append(tokenized_sentences[sent_index]['s_id'])\n",
    "\n",
    "            founds.append(is_found)\n",
    "            is_substrings.append(is_substring)\n",
    "            doc_texts.append(texts[text_index].text)\n",
    "            ids.append(texts[text_index].attrib['id'])\n",
    "            s_ids.append(tokenized_sentences[sent_index]['s_id'])\n",
    "            srcs.append(tokenized_sentences[sent_index]['src'])\n",
    "            tgts.append(tokenized_sentences[sent_index]['tgt'])\n",
    "\n",
    "    df = pd.DataFrame(list(zip(founds, is_substrings, doc_texts, ids, s_ids, srcs, tgts)), \n",
    "                                      columns =['found', 'substr', 'doc_text', 'doc_id', 's_id', 'src', 'tgt'])\n",
    "    return df.loc[(df['substr'] == 0) & (df['found'] == True)].reset_index(drop=True), \\\n",
    "        df.loc[(df['substr'] == -1) & (df['found'] == True)].reset_index(drop=True), \\\n",
    "        df.loc[(df['substr'] == 1) & (df['found'] == True)].reset_index(drop=True), \\\n",
    "        df.loc[(df['found'] == False)].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        s = difflib.SequenceMatcher(lambda x: x == '.', s1, s2)\n",
    "        pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "        return s1[pos_a:pos_a+size], pos_a, (pos_a+size), size/len(s1), s.ratio()\n",
    "\n",
    "    s = difflib.SequenceMatcher(None, s2, s1)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s2), 0, len(s1)) \n",
    "    return s2[pos_a:pos_a+size], pos_a, (pos_a+size), size/len(s2), s.ratio()\n",
    "\n",
    "def get_filtered_dfs(tokenized_sentences, texts):\n",
    "    percent_threshold = 0.3\n",
    "    ratio_threshold   = 0.5\n",
    "\n",
    "    overlaps = []\n",
    "    percents = []\n",
    "    ratios   = []\n",
    "    docs     = []\n",
    "    srcs     = []\n",
    "    tgts     = []\n",
    "    ids      = []\n",
    "    s_ids    = []\n",
    "\n",
    "    for sent_index in range(len(tokenized_sentences)):\n",
    "        for text_index in range(len(texts)):\n",
    "            doc_text       = texts[text_index].text\n",
    "            tokenized_text = tokenized_sentences[sent_index]['src']\n",
    "            overlap, start, end, percent, ratio = get_overlap(doc_text, tokenized_text)\n",
    "\n",
    "            overlaps.append(overlap)\n",
    "            percents.append(percent)\n",
    "            ratios.append(ratio)\n",
    "            docs.append(texts[text_index].text)\n",
    "            ids.append(texts[text_index].attrib['id'])\n",
    "            srcs.append(tokenized_sentences[sent_index]['src'])\n",
    "            tgts.append(tokenized_sentences[sent_index]['tgt'])\n",
    "            s_ids.append(tokenized_sentences[sent_index]['s_id'])\n",
    "\n",
    "    df = pd.DataFrame(list(zip(percents, ratios, overlaps, docs, srcs, tgts, s_ids, ids)), \n",
    "                                          columns =['percent', 'ratio', 'overlap', 'doc_text', 'src', 'tgt', 's_id', 'doc_id'])\n",
    "\n",
    "    filtered_df = df.loc[(df['percent'] >= percent_threshold ) & (df['ratio'] >= ratio_threshold)]\n",
    "    pm_df       = filtered_df.loc[(filtered_df['percent'] == 1.0 ) & (filtered_df['ratio'] == 1.0)]\n",
    "    pm_df1      = filtered_df[~filtered_df.index.isin(pm_df.index)]\n",
    "    return pm_df, pm_df1, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_dir, filenames = extract_docx(input_filepath, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document has (51) text tags, tokenized sentences (85)\n",
      "filtered match (40)\n",
      "perfect match (48), tokenized match (30) substring match (9) multiple match (21)\n"
     ]
    }
   ],
   "source": [
    "document_xml             = get_xmltree(os.path.join(extracted_dir, 'word', 'document.xml'))\n",
    "document_xml             = update_document_runs(document_xml)\n",
    "texts                    = get_text_tags(document_xml)\n",
    "\n",
    "tokenized_sentences      = get_tokenized_sentences(fetch_content_filepath)\n",
    "print('document has (%d) text tags, tokenized sentences (%d)' % (len(texts), len(tokenized_sentences)))\n",
    "\n",
    "pm_df, filtered_df, df   = get_filtered_dfs(tokenized_sentences, texts)\n",
    "tm_df                    = get_updated_tokenized_text_matched(filtered_df)\n",
    "substring_df, multiple_df = get_updated_document_text_matched(filtered_df)\n",
    "\n",
    "df.to_csv('df.csv')\n",
    "filtered_df.to_csv('filtered_df.csv')\n",
    "pm_df.to_csv('pm_df.csv')\n",
    "tm_df.to_csv('tm_df.csv')\n",
    "substring_df.to_csv('substring_df.csv')\n",
    "multiple_df.to_csv('multiple_df.csv')\n",
    "\n",
    "print('filtered match (%d)' % (len(filtered_df)))\n",
    "print('perfect match (%d), tokenized match (%d) substring match (%d) multiple match (%d)' \\\n",
    "      % (len(pm_df), len(tm_df), len(substring_df), len(multiple_df)))\n",
    "# pm_df, tm_df, dtm_df, df = get_matched_dfs(tokenized_sentences, texts)\n",
    "# print('perfect match (%d), tokenized sentences matched (%d), document text matched (%d)' % (len(pm_df), len(tm_df), len(dtm_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " replace and create translated sentence\n",
    "'''\n",
    "replace_translated_df(pm_df, texts)\n",
    "# replace_translated_df(tm_df, texts)\n",
    "# replace_translated_df(substring_df, texts)\n",
    "# replace_translated_df(multiple_df, texts)\n",
    "\n",
    "# replace_translated_df(get_updated_tokenized_text_matched(tm_df), texts)\n",
    "# substring_df, multiple_df = get_updated_document_text_matched(dtm_df)\n",
    "\n",
    "# if substring_df.empty == False:\n",
    "#     replace_translated_df(substring_df, texts)\n",
    "    \n",
    "# if multiple_df.empty != False:\n",
    "#     replace_translated_df(multiple_df, texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = get_specific_tags(document_xml, 'p')\n",
    "for p in ps:\n",
    "    update_font_property(p, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_document_xml(extracted_dir, document_xml)\n",
    "save_docx(extracted_dir, filenames, os.path.join(output_dir, translated_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_text = 'recruited prior to change of policy as (ii) aforesaid. The Permanent Commission shall be offered to them after completion of five years. They would also be entitled to all consequential benefits such as promotion and other financial benefits. However, the aforesaid benefits are to be made available only to women officers in service or who have approached this Court by filing these petitions and have retired during the course of pendency of the petitions.'\n",
    "# tokenized_text = 'This benefit would be conferred to women officers recruited prior to change of policy as (ii) aforesaid.'\n",
    "\n",
    "tokenized_text = 'The original ToE provided for a contractual period of five years after which the officers were to be released from service. The officers who were granted commission under the Army instruction were not entitled to PC or to any extension beyond five years of commissioned service.'\n",
    "doc_text = '8. The original ToE provided for a contractual period of five years after which the officers were to be released from service.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap_v1(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        return get_diff(s1, s2)\n",
    "    else:\n",
    "        return get_diff(s2, s1)\n",
    "\n",
    "def get_diff(s1, s2):\n",
    "    s      = difflib.SequenceMatcher(lambda x: x == '.', s1, s2)\n",
    "    pos_s1, pos_s2, match_size = s.find_longest_match(0, len(s1), 0, len(s2))\n",
    "\n",
    "    return s1[pos_s1:pos_s1+match_size], pos_s1, pos_s2, \\\n",
    "            len(s1[pos_s1:pos_s1+match_size])/len(s1), \\\n",
    "            len(s2[pos_s2:pos_s2+match_size])/len(s2), \\\n",
    "            s.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1999', 3, 0, 0.5714285714285714, 1.0, 0.7272727272727273)\n"
     ]
    }
   ],
   "source": [
    "print(get_overlap_v1('1. 1999', '1999'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1999', 3, 0, 0.5714285714285714, 1.0, 0.7272727272727273)\n"
     ]
    }
   ],
   "source": [
    "print(get_overlap_v1('1999', '1. 1999'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('cv3': conda)",
   "language": "python",
   "name": "python35664bitcv3conda56b31b492c17456d86703f6408b0e697"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
